{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10852132,"sourceType":"datasetVersion","datasetId":6740184},{"sourceId":10868665,"sourceType":"datasetVersion","datasetId":6752136}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/joymachadolab/brandmr-model-usecase-demo?scriptVersionId=248515517\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"! pip install huggingface\n! pip install huggingface-hub\n! pip install dataset\n! pip install transformer\n! pip install bitsandbytes","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-06T10:18:29.042023Z","iopub.execute_input":"2025-03-06T10:18:29.042325Z","iopub.status.idle":"2025-03-06T10:18:50.601607Z","shell.execute_reply.started":"2025-03-06T10:18:29.042291Z","shell.execute_reply":"2025-03-06T10:18:50.600501Z"}},"outputs":[{"name":"stdout","text":"Collecting huggingface\n  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\nDownloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\nInstalling collected packages: huggingface\nSuccessfully installed huggingface-0.0.1\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.29.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2024.12.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2025.1.31)\nCollecting dataset\n  Downloading dataset-1.6.2-py2.py3-none-any.whl.metadata (1.9 kB)\nCollecting sqlalchemy<2.0.0,>=1.3.2 (from dataset)\n  Downloading SQLAlchemy-1.4.54-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: alembic>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from dataset) (1.14.1)\nCollecting banal>=1.0.1 (from dataset)\n  Downloading banal-1.0.6-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=0.6.2->dataset) (1.3.9)\nRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=0.6.2->dataset) (4.12.2)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<2.0.0,>=1.3.2->dataset) (3.1.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=0.6.2->dataset) (3.0.2)\nDownloading dataset-1.6.2-py2.py3-none-any.whl (18 kB)\nDownloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\nDownloading SQLAlchemy-1.4.54-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: banal, sqlalchemy, dataset\n  Attempting uninstall: sqlalchemy\n    Found existing installation: SQLAlchemy 2.0.36\n    Uninstalling SQLAlchemy-2.0.36:\n      Successfully uninstalled SQLAlchemy-2.0.36\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\nlangchain 0.3.12 requires async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\", but you have async-timeout 5.0.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed banal-1.0.6 dataset-1.6.2 sqlalchemy-1.4.54\n\u001b[31mERROR: Could not find a version that satisfies the requirement transformer (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for transformer\u001b[0m\u001b[31m\n\u001b[0mCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import login \nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n# Import kaggle secrets\nfrom kaggle_secrets import UserSecretsClient\nimport pandas as pd\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T10:18:50.602627Z","iopub.execute_input":"2025-03-06T10:18:50.602896Z","iopub.status.idle":"2025-03-06T10:18:58.694635Z","shell.execute_reply.started":"2025-03-06T10:18:50.602844Z","shell.execute_reply":"2025-03-06T10:18:58.693961Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\n\nhf = user_secrets.get_secret(\"huggingface\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T10:18:58.695448Z","iopub.execute_input":"2025-03-06T10:18:58.695821Z","iopub.status.idle":"2025-03-06T10:18:58.902222Z","shell.execute_reply.started":"2025-03-06T10:18:58.695801Z","shell.execute_reply":"2025-03-06T10:18:58.901617Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"max_seq_length = 2048\ndtype = None\nload_in_4bit = True\n\ntokenizer = AutoTokenizer.from_pretrained(\"jmachado-lab/DeepSeek-R1-BrandMR-casetype-classification\")\nmodel = AutoModelForCausalLM.from_pretrained(\"jmachado-lab/DeepSeek-R1-BrandMR-casetype-classification\", trust_remote_code=True, low_cpu_mem_usage=True).to(\"cuda\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T10:18:58.904118Z","iopub.execute_input":"2025-03-06T10:18:58.904315Z","iopub.status.idle":"2025-03-06T10:19:52.35803Z","shell.execute_reply.started":"2025-03-06T10:18:58.904298Z","shell.execute_reply":"2025-03-06T10:19:52.357102Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4582add88532496c9112e63cb6fb7aac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d3a0487027c45d0ad97d229c9de2958"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a26c6143655d4a598d9c71ea25158ff4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"290f2078537142529b01e8109818dadf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/824 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc41dd678aae4c7b99faad9f0650bcd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fdcb183c9ba477bb24d7bf009ad5cb1"}},"metadata":{}},{"name":"stderr","text":"Exception ignored in: <function _xla_gc_callback at 0x7a42b33a24d0>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n    def _xla_gc_callback(*args):\nKeyboardInterrupt: \nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"651d9c732a68497fa697163067e5fc98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e5c9197b5d0479e9b427a2d4e547eef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d852d36e63ef4f2dbaa31010b28d9c65"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T10:19:52.359561Z","iopub.execute_input":"2025-03-06T10:19:52.360182Z","iopub.status.idle":"2025-03-06T10:19:52.375486Z","shell.execute_reply.started":"2025-03-06T10:19:52.360158Z","shell.execute_reply":"2025-03-06T10:19:52.374591Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n    (layers): ModuleList(\n      (0): LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (k_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=1024, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (v_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=1024, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (o_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=14336, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (up_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=14336, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (down_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=14336, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n      (1): LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (k_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=1024, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (v_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=1024, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (o_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): lora.Linear(\n            (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=14336, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (up_proj): lora.Linear(\n            (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=14336, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (down_proj): lora.Linear(\n            (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=14336, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n      (2-31): 30 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (k_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=1024, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (v_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=1024, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (o_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=14336, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (up_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=14336, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (down_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=14336, out_features=16, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=16, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\nWrite a response that appropriately completes the request.\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are an intake employee for a Dutch legal firm expert with advanced knowledge in assigning incoming requests from users to the right kind of case types set by the firm.\nThe case types set by the legal firm are listed as: {}\nAnswer the following question sent in by a user in the Dutch language and classify the user question to the right kind of case type set by the firm along with your chain of thought. \nIf there is more than 1 case type that's relevant to a situation, pick the one that is \nmost relevant by reasoning why and stick to the case types set by the firm.\nAssign atleast 1 case type with reasoning\n\n### Question:\n{}\n\n### Response:\n<think>\n{}\n</think>\n{}\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T10:23:21.257501Z","iopub.execute_input":"2025-03-06T10:23:21.257774Z","iopub.status.idle":"2025-03-06T10:23:21.273556Z","shell.execute_reply.started":"2025-03-06T10:23:21.257743Z","shell.execute_reply":"2025-03-06T10:23:21.272929Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"df = pd.read_excel(\"../input/casetype-dataset/casetype_dataset.xlsx\")\ncolumn_values = set(df[\"Zaaktype benaming\"])\ncase_type_list = \", \".join(map(str, column_values))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T10:24:26.556814Z","iopub.execute_input":"2025-03-06T10:24:26.557129Z","iopub.status.idle":"2025-03-06T10:24:26.594262Z","shell.execute_reply.started":"2025-03-06T10:24:26.557104Z","shell.execute_reply":"2025-03-06T10:24:26.593587Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"question = \"Ik was een werknemer in een winkel en ik werd beschuldigd van diefstal en ontslagen van mijn baan. Ik heb niets gestolen, daar ben ik zeker van. Kunt u mij helpen?\"\n\ninputs = tokenizer([train_prompt_style.format(case_type_list, question, \"\", \"\")], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs)\nvalid_response = response[0].split(\"### Response:\")[1]\nprint(valid_response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T10:23:43.497967Z","iopub.execute_input":"2025-03-06T10:23:43.498305Z","iopub.status.idle":"2025-03-06T10:24:02.493964Z","shell.execute_reply.started":"2025-03-06T10:23:43.498273Z","shell.execute_reply":"2025-03-06T10:24:02.493169Z"}},"outputs":[{"name":"stdout","text":"\n<think>\n\n</think>\n\nDe beschuldiging van diefstal en ontslag op staande voet is een arbeidsrechtelijke kwestie. Dit past onder de AS ARBEIDSRECHT Overig, omdat het gaat om een arbeidsrelatie waarbij de werkgever een ontslag pleegt. Hier zijn geen civielrechtelijke procedures aan de hand, maar de focus is op de arbeidsrechtelijke behandeling van de situatie.<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"question = \"Ik was werknemer in een winkel en werd beschuldigd van diefstal en op staande voet ontslagen. Ik heb niets gestolen, daar ben ik zeker van. Kunt u mij helpen?\"\n\ninputs = tokenizer([train_prompt_style.format(case_type_list, question, \"\", \"\")], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs)\nvalid_response = response[0].split(\"### Response:\")[1]\nprint(valid_response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T10:24:30.787114Z","iopub.execute_input":"2025-03-06T10:24:30.787397Z","iopub.status.idle":"2025-03-06T10:24:46.492282Z","shell.execute_reply.started":"2025-03-06T10:24:30.787376Z","shell.execute_reply":"2025-03-06T10:24:46.491529Z"}},"outputs":[{"name":"stdout","text":"\n<think>\n\n</think>\n\nIk was werknemer in een winkel en werd beschuldigd van diefstal en op staande voet ontslagen. Ik heb niets gestolen, daar ben ik zeker van. Kunt u mij helpen?\n\nAS ARBEIDSRECHT Ontslag op staande voet<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"question = \"Ik heb via een website een schilder ingehuurd voor een dag om mijn huis te schilderen en heb de helft vooruitbetaald, maar hij is niet gekomen of heeft niet op mijn bericht gereageerd. Kunt u mij helpen mijn geld terug te krijgen van het bedrijf waar hij via is ingehuurd?\"\n\ninputs = tokenizer([train_prompt_style.format(case_type_list, question, \"\", \"\")], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs)\nvalid_response = response[0].split(\"### Response:\")[1]\nprint(valid_response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T10:24:46.493398Z","iopub.execute_input":"2025-03-06T10:24:46.493721Z","iopub.status.idle":"2025-03-06T10:25:11.434953Z","shell.execute_reply.started":"2025-03-06T10:24:46.493687Z","shell.execute_reply":"2025-03-06T10:25:11.433783Z"}},"outputs":[{"name":"stdout","text":"\n<think>\n\n</think>\n\nIk heb via een website een schilder ingehuurd voor een dag om mijn huis te schilderen en heb de helft vooruitbetaald, maar hij is niet gekomen of heeft niet op mijn bericht gereageerd. Kunt u mij helpen mijn geld terug te krijgen van het bedrijf waar hij via is ingehuurd?\n\nDe situatie betreft een contractueel geschil over een niet voltooide dienst, waarbij geld is betaald voor een dienst die niet is ontvangen. Dit past onder het case type CA gekochte dienst. \n\n**Classificatie:** CA gekochte dienst<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"question = \"Ik heb een schilder ingehuurd om een ​​kamer in mijn huis te schilderen. We hebben e-mails heen en weer gestuurd en ik heb hem het voorschot rechtstreeks op zijn bankrekening gestort. Op de dag van de overeenkomst kwam hij niet opdagen en beantwoordde mijn telefoontjes niet. Is er iets wat we kunnen doen?\"\n\ninputs = tokenizer([train_prompt_style.format(case_type_list, question, \"\", \"\")], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs)\nvalid_response = response[0].split(\"### Response:\")[1]\nprint(valid_response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T10:21:22.204164Z","iopub.execute_input":"2025-03-06T10:21:22.204473Z","iopub.status.idle":"2025-03-06T10:21:49.250504Z","shell.execute_reply.started":"2025-03-06T10:21:22.204443Z","shell.execute_reply":"2025-03-06T10:21:49.249509Z"}},"outputs":[{"name":"stdout","text":"\n<think>\n\n</think>\n\nIk heb een schilder ingehuurd om een kamer te schilderen. Na de overeenkomst, waarop ik hem het voorschot rechtstreeks op zijn bankrekening overmaakte, kwam hij niet opdagen en beantwoordde mijn telefoontjes niet. Dit is vervelend, maar kan ik hier rechtelijke actie tegen taken? \n\nIk denk dat hier een conflict bestaat tussen de schilder en mij. Omdat de schilder de overeenkomst niet nakomt, kan ik mogelijk een geschil hebben met hem. Dit zou kunnen worden geklasseerd onder de categorie **CA gekochte dienst**, omdat de dienst niet is geleverd zoals overeengekomen is.<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"question = \"Ik heb een auto gekocht bij een erkende dealer, binnen een paar dagen na mijn aankoop, merkte ik problemen met mijn auto, waaronder problemen met de motor. Wat kan ik doen om de auto terug te brengen naar de dealer?\"\n\ninputs = tokenizer([train_prompt_style.format(case_type_list, question, \"\", \"\")], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs)\nvalid_response = response[0].split(\"### Response:\")[1]\nprint(valid_response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T10:21:49.251391Z","iopub.execute_input":"2025-03-06T10:21:49.251648Z","iopub.status.idle":"2025-03-06T10:21:58.518795Z","shell.execute_reply.started":"2025-03-06T10:21:49.251626Z","shell.execute_reply":"2025-03-06T10:21:58.518036Z"}},"outputs":[{"name":"stdout","text":"\n<think>\n\n</think>\n\nCA Auto<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"question = \"Het plafond van mijn huurhuis lekt en mijn verhuurder zou het moeten repareren, maar hij doet niets om het te repareren zoals in ons huurcontract staat. Welke stappen kan ik ondernemen om hem het probleem te laten oplossen?\"\n\ninputs = tokenizer([train_prompt_style.format(case_type_list, question, \"\", \"\")], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs)\nvalid_response = response[0].split(\"### Response:\")[1]\nprint(valid_response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T10:21:58.520949Z","iopub.execute_input":"2025-03-06T10:21:58.521192Z","iopub.status.idle":"2025-03-06T10:22:19.578121Z","shell.execute_reply.started":"2025-03-06T10:21:58.521171Z","shell.execute_reply":"2025-03-06T10:22:19.577308Z"}},"outputs":[{"name":"stdout","text":"\n<think>\n\n</think>\n\nIk kan een ingebouwde opstelling maken aan de verhuurder om het plafond te laten repareren, zoals het contract meemaakt. Dit is de eerste stap. Als de verhuurder niet reageert, kan ik een aanrijding naar de rechtbank maken om het probleem te laten oplossen. Dit past onder de categorie WO Bouwrecht Aanneming van werk, omdat het gaat om een verplicht werk dat de verhuurder niet volgens het contract uitvoert.<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"question = \"Ik heb een conflict met de verkoper van een tweedehands boot. Wat kan ik doen?\"\n\ninputs = tokenizer([train_prompt_style.format(case_type_list, question, \"\", \"\")], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs)\nvalid_response = response[0].split(\"### Response:\")[1]\nprint(valid_response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T10:22:19.579044Z","iopub.execute_input":"2025-03-06T10:22:19.579263Z","iopub.status.idle":"2025-03-06T10:22:32.218821Z","shell.execute_reply.started":"2025-03-06T10:22:19.579244Z","shell.execute_reply":"2025-03-06T10:22:32.21805Z"}},"outputs":[{"name":"stdout","text":"\n<think>\n\n</think>\n\nIk heb een conflict met de verkoper van een tweedehands boot. Wat kan ik doen?\n\n### Case Type:\nCA Consumentengeschil Verkopen en kopen<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"question = \"Ik ben ontslagen bij mijn baan in de VS. Kan ik juridische stappen ondernemen tegen mijn bedrijf?\"\n\ninputs = tokenizer([train_prompt_style.format(case_type_list, question, \"\", \"\")], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs)\nvalid_response = response[0].split(\"### Response:\")[1]\nprint(valid_response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T10:22:32.219562Z","iopub.execute_input":"2025-03-06T10:22:32.219796Z","iopub.status.idle":"2025-03-06T10:23:02.935372Z","shell.execute_reply.started":"2025-03-06T10:22:32.219777Z","shell.execute_reply":"2025-03-06T10:23:02.934543Z"}},"outputs":[{"name":"stdout","text":"\n<think>\n\n</think>\n\nDe ontslag in de VS heeft gevolgen voor mijn arbeidsrechtelijke situatie in de NL. Ik wil juridische stappen ondernemen tegen mijn bedrijf, maar ik ben onzeker over de geschilpunten en procedure. \n\nAangezien ik een Amerikaans ontslag heb, moet ik overwegen waar dit ontslag is geregeld en of dit rechtstreek op de NL arbeidsrechten van toepassing is. Het is mogelijk dat mijn rechten in de NL worden beïntefsd door het buitenlandse ontslag. \n\nIk denk dat het AS ARBEIDSRECHT ziekte / arbeidsongeschiktheid zou kunnen zijn, omdat ik een ontslag op staande voet heb, maar ik ben niet ziek. Dit type case kan helpen om mijn rechten te versterken in de context van het ontslag in de VS.<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"question = \"Ik heb reisplannen gemaakt met een bedrijf in NL naar Mexico. Op het laatste moment hebben ze mijn plannen geannuleerd zonder goede reden. Ik heb geld uitgegeven, behalve het betalen van het reisbedrijf om de reis te plannen. Kan ik mijn geld terugkrijgen?\"\n\ninputs = tokenizer([train_prompt_style.format(case_type_list, question, \"\", \"\")], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs)\nvalid_response = response[0].split(\"### Response:\")[1]\nprint(valid_response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T10:23:02.936144Z","iopub.execute_input":"2025-03-06T10:23:02.936382Z","iopub.status.idle":"2025-03-06T10:23:21.249571Z","shell.execute_reply.started":"2025-03-06T10:23:02.936362Z","shell.execute_reply":"2025-03-06T10:23:21.248671Z"}},"outputs":[{"name":"stdout","text":"\n<think>\n\n</think>\n\nIk heb reisplannen gemaakt met een bedrijf in NL naar Mexico. Op het laatste moment hebben ze mijn plannen geannuleerd zonder goede reden. Ik heb geld uitgegeven, behalve het betalen van het reisbedrijf om de reis te plannen. Kan ik mijn geld terugkrijgen?\n\nCA gekochte dienst<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"question = \"Ik ben vandaag ontslagen\"\n\ninputs = tokenizer([train_prompt_style.format(case_type_list, question, \"\", \"\")], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs)\nvalid_response = response[0].split(\"### Response:\")[1]\nprint(valid_response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T10:28:15.441828Z","iopub.execute_input":"2025-03-06T10:28:15.442182Z","iopub.status.idle":"2025-03-06T10:28:26.429211Z","shell.execute_reply.started":"2025-03-06T10:28:15.442155Z","shell.execute_reply":"2025-03-06T10:28:26.428128Z"}},"outputs":[{"name":"stdout","text":"\n<think>\n\n</think>\n\nIk ben vandaag ontslagen  \nAS ARBEIDSRECHT Ontslag UWV<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"question = \"I have to fire a worker today, what steps can I take\"\n\ninputs = tokenizer([train_prompt_style.format(case_type_list, question, \"\", \"\")], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(\n    input_ids=inputs.input_ids,\n    attention_mask=inputs.attention_mask,\n    max_new_tokens=1200,\n    use_cache=True,\n)\nresponse = tokenizer.batch_decode(outputs)\nvalid_response = response[0].split(\"### Response:\")[1]\nprint(valid_response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T10:29:17.002208Z","iopub.execute_input":"2025-03-06T10:29:17.00252Z","iopub.status.idle":"2025-03-06T10:29:39.512124Z","shell.execute_reply.started":"2025-03-06T10:29:17.002484Z","shell.execute_reply":"2025-03-06T10:29:39.511338Z"}},"outputs":[{"name":"stdout","text":"\n<think>\n\n</think>\n\nDe ontslag van een werknemer is een serieus onderdeel en moet volgens de arbeidsrechtelijke regels worden behandeld. Als werkgever moet je een duidelijke procedure volgen, waaronder het toesturen van een ontslagbrief met de daad verkeerende omstandigheden, het overleggen van de arbeidsvoorwaarden en het bestek van een ontslag op staande voet of via UWV. Raadpleeg een arbeidsrechtexpert voor een precieze behandeling van jouw specifieke situatie.<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":29}]}